---
sidebar_position: 2
---

# Decisiones de DiseÃ±o en Sistemas IA

**Â¿REST, GraphQL, o gRPC? Â¿API de OpenAI o self-hosted? Â¿SQL o vector database?**

Las decisiones arquitectÃ³nicas en sistemas IA no son opcionales. Un error aquÃ­ multiplica los problemas downstream. Esta guÃ­a te ayuda a tomar decisiones fundamentadas.

## ğŸ—ï¸ API Design: Â¿CÃ³mo Expones tus Agentes?

### REST: Simple pero Limitado
```typescript
// Para agentes simples con llamadas directas
POST /api/agents/analyze
Content-Type: application/json

{
  "task": "analyze codebase",
  "files": ["src/**/*.ts"],
  "output": "summary"
}
```

**CuÃ¡ndo usar REST:**
- Agentes con interfaces predecibles
- Equipos pequeÃ±os (< 5 personas)
- Prototipos rÃ¡pidos
- IntegraciÃ³n con sistemas legacy

**Tradeoffs:**
âœ… Simple de entender y debuggear
âœ… Herramientas maduras (Postman, curl)
âŒ Streaming limitado para respuestas largas
âŒ Schema evolution complicado

### GraphQL: Flexible pero Complejo
```graphql
# Para agentes que necesitan queries complejas
query AnalyzeRepository($owner: String!, $repo: String!) {
  agent(task: "security_audit") {
    findings(type: VULNERABILITY) {
      file
      line
      severity
      description
    }
    metrics {
      complexity
      coverage
    }
  }
}
```

**CuÃ¡ndo usar GraphQL:**
- Interfaces dinÃ¡micas segÃºn el tipo de agente
- Equipos medianos (5-15 personas)
- Productos con mÃºltiples consumidores
- EvoluciÃ³n frecuente de requirements

**Tradeoffs:**
âœ… Queries precisas, menos over/under-fetching
âœ… Schema fuerte con type safety
âŒ Complejidad inicial alta
âŒ Caching mÃ¡s complicado

### gRPC: Performance pero Verbose
```protobuf
// Para agentes de alta performance
service AgentService {
  rpc AnalyzeStream(stream AnalysisRequest) returns (stream AnalysisResponse);
}

message AnalysisRequest {
  string task = 1;
  repeated string files = 2;
  AnalysisOptions options = 3;
}
```

**CuÃ¡ndo usar gRPC:**
- Agentes con streaming en tiempo real
- Microservicios con agentes especializados
- Equipos grandes (> 15 personas)
- Requisitos de baja latencia

**Tradeoffs:**
âœ… Mejor performance que REST/GraphQL
âœ… Streaming nativo bidireccional
âŒ Debugging mÃ¡s difÃ­cil
âŒ Herramientas menos maduras

## ğŸ—„ï¸ Storage: Â¿DÃ³nde Guardas el Conocimiento?

### Vector Databases: Para Contexto SemÃ¡ntico
```typescript
// Pinecone, Weaviate, Qdrant
const vectorStore = new PineconeStore({
  apiKey: process.env.PINECONE_API_KEY,
  indexName: 'agent-knowledge'
});

// Embedding del cÃ³digo para bÃºsqueda semÃ¡ntica
const embedding = await openai.embeddings.create({
  model: "text-embedding-3-small",
  input: codeSnippet
});

await vectorStore.addDocuments([
  new Document({
    pageContent: codeSnippet,
    metadata: { file: 'src/agent.ts', type: 'function' },
    embedding
  })
]);
```

**CuÃ¡ndo usar Vector DB:**
- RAG (Retrieval-Augmented Generation)
- BÃºsqueda semÃ¡ntica en cÃ³digo/documentos
- Agentes que necesitan contexto amplio
- Sistemas de recomendaciÃ³n

### SQL Databases: Para Datos Estructurados
```sql
-- PostgreSQL con pgvector para hÃ­brido
CREATE TABLE agent_memories (
  id UUID PRIMARY KEY,
  agent_id VARCHAR(50) NOT NULL,
  type VARCHAR(20) NOT NULL, -- 'conversation', 'learning', 'error'
  content TEXT NOT NULL,
  embedding vector(1536), -- OpenAI ada-002
  metadata JSONB,
  created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX ON agent_memories USING ivfflat (embedding vector_cosine_ops);
```

**CuÃ¡ndo usar SQL:**
- Datos relacionales (usuarios, proyectos, permisos)
- Queries complejas con joins
- Transacciones ACID
- Reporting y analytics

### Redis: Para Cache y Sesiones
```typescript
// Cache de embeddings frecuentes
const cacheKey = `embedding:${hash(codeSnippet)}`;
const cached = await redis.get(cacheKey);
if (!cached) {
  const embedding = await openai.embeddings.create({...});
  await redis.setex(cacheKey, 3600, JSON.stringify(embedding)); // 1 hora
}
```

**CuÃ¡ndo usar Redis:**
- Cache de resultados costosos
- Sesiones de agentes
- Rate limiting
- Pub/sub entre agentes

## ğŸ¤– LLM Selection: Â¿API o Self-Hosted?

### API Services: FÃ¡cil pero Dependiente
```typescript
// OpenAI/Claude APIs
const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

const response = await client.chat.completions.create({
  model: "gpt-4-turbo-preview",
  messages: [{ role: "user", content: task }],
  tools: agentTools,
  temperature: 0.1 // Baja para consistencia
});
```

**CuÃ¡ndo usar APIs:**
- Prototipos y MVPs
- Equipos pequeÃ±os sin infra expertise
- Presupuesto flexible
- Modelos actualizados automÃ¡ticamente

**Tradeoffs:**
âœ… Cero mantenimiento de modelos
âœ… Modelos state-of-the-art
âŒ Costos variables impredecibles
âŒ Dependencia de terceros
âŒ Rate limits y downtime

### Self-Hosted: Control pero Complejidad
```yaml
# Docker Compose para vLLM
version: '3.8'
services:
  vllm:
    image: vllm/vllm-openai:latest
    ports:
      - "8000:8000"
    environment:
      - MODEL_NAME=meta-llama/Llama-2-70b-chat-hf
      - TOKENIZERS_PARALLELISM=false
    volumes:
      - ./models:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
```

**CuÃ¡ndo usar Self-Hosted:**
- Aplicaciones crÃ­ticas con SLA estrictos
- Datos sensibles (compliance)
- Costos predecibles a largo plazo
- Equipos con expertise en MLOps

**Tradeoffs:**
âœ… Control total sobre modelos y datos
âœ… Costos predecibles
âŒ Mantenimiento complejo
âŒ Hardware costoso (GPUs)
âŒ Model updates manuales

## ğŸš€ Deployment: Â¿Serverless o Containers?

### Serverless: Escalado AutomÃ¡tico
```yaml
# AWS Lambda para agentes simples
functions:
  analyzeCode:
    handler: src/handlers/analyze.handler
    runtime: nodejs20.x
    timeout: 900 # 15 minutos mÃ¡ximo
    memorySize: 2048
    environment:
      OPENAI_API_KEY: ${env:OPENAI_API_KEY}
```

**CuÃ¡ndo usar Serverless:**
- Agentes con carga variable
- Equipos sin DevOps
- Costos basados en uso real
- Prototipos rÃ¡pidos

**Tradeoffs:**
âœ… Escalado automÃ¡tico
âœ… Cero mantenimiento de servers
âŒ Timeouts estrictos (15 min Lambda)
âŒ Cold starts
âŒ Vendor lock-in

### Containers: Control Completo
```dockerfile
# Multi-stage para agentes optimizados
FROM node:20-alpine AS builder
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

FROM node:20-alpine AS runtime
RUN apk add --no-cache dumb-init
USER node
WORKDIR /app
COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/dist ./dist

EXPOSE 3000
ENTRYPOINT ["dumb-init", "--"]
CMD ["node", "dist/server.js"]
```

**CuÃ¡ndo usar Containers:**
- Agentes complejos con dependencias
- Equipos con Kubernetes expertise
- Aplicaciones stateful
- Requisitos de compliance especÃ­ficos

**Tradeoffs:**
âœ… Control total del runtime
âœ… Portabilidad entre clouds
âŒ OrquestaciÃ³n compleja (Kubernetes)
âŒ Costos de gestiÃ³n de infraestructura

## ğŸ¯ Decision Tree: Â¿QuÃ© Elegir SegÃºn tu Caso?

```
Â¿Equipo pequeÃ±o (< 5 devs) Y prototipo?
â”œâ”€â”€ SÃ â†’ REST + API services + Serverless
â””â”€â”€ NO â†’ Â¿Datos sensibles o compliance?
    â”œâ”€â”€ SÃ â†’ gRPC + Self-hosted + Containers
    â””â”€â”€ NO â†’ Â¿Performance crÃ­tica?
        â”œâ”€â”€ SÃ â†’ GraphQL + Vector DB + Self-hosted + Containers
        â””â”€â”€ NO â†’ REST + SQL + API services + Serverless
```

## âš ï¸ Decisiones que te ArrepentirÃ¡s

- **Empezar con API sin plan de migraciÃ³n**: Los costos se disparan
- **Vector DB para todo**: No todo es bÃºsqueda semÃ¡ntica
- **Serverless para agentes stateful**: Pierdes estado entre llamadas
- **Self-hosted sin MLOps**: Actualizaciones de modelos son un infierno

**Recuerda**: Las decisiones arquitectÃ³nicas se pagan caro cambiar despuÃ©s. Toma tiempo para evaluar tradeoffs antes de comprometerte.
